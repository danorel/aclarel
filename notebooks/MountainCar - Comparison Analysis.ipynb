{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ca45a75-f165-4ea3-bc90-82975e6a1ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/danorel/Workspace/Education/University/KMA/Research/aclarel\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39deedfb-bba0-4411-ba29-682770bd9115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import pathlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import environments.mountain_car.environment as mountain_car\n",
    "import environments.mountain_car.experiments as experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2578746b-32c2-4d99-97c1-ddb34ac23df5",
   "metadata": {},
   "source": [
    "## Curriculum Learning: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fe9e13-c218-4b3b-a556-4cadca967a73",
   "metadata": {},
   "source": [
    "### Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1da28ed2-c431-4eaf-8c1b-06379ed1223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def data_frame_from_agents(agents):\n",
    "    df = pd.DataFrame()\n",
    "    for agent in agents:\n",
    "        df = pd.concat([df, agent.measurements])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0177b7c8-d10a-4258-976e-b39bcf5e148b",
   "metadata": {},
   "source": [
    "## Reinforcement Learning: Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e36bc6a-1ea4-4cec-b410-2a53cee6dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_DIR = pathlib.Path(\"datasets\") / \"mountain_car\"\n",
    "DATASETS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab04375-6ae4-4a04-a7ce-095a0e7c8d4a",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a7c7c-63be-44ec-ac45-8562ec58e8ba",
   "metadata": {},
   "source": [
    "#### Curriculum parameter: gravity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "033bae37-cc5f-4a67-90e4-0985d5940885",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Device: cpu\n",
      "Device: cpu\n",
      "Device: cpu\n",
      "Device: cpu\n",
      "Device: cpu\n",
      "Device: cpu\n",
      "Device: cpu\n",
      "Device: cpu\n",
      "Device: cpu\n",
      "Device: cpu\n",
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                            | 0/12 [00:00<?, ?it/s]\n",
      "  0%|                                                          | 0/1001 [00:00<?, ?it/s]\u001b[A/Users/danorel/miniconda3/envs/aclarel_3.11.3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/Users/danorel/Workspace/Education/University/KMA/Research/aclarel/environments/mountain_car/rl_methods/__init__.py:64: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.measurements = pd.concat([self.measurements, measurement], ignore_index=True)\n",
      "\n",
      "  0%|                                                | 1/1001 [00:04<1:07:56,  4.08s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation 0 (Epsilon=0.29406):\n",
      "\tTraining Gravity: 0.0025\n",
      " \tTraining Stability: 0.0\n",
      " \tAAR: -10.0\n",
      " \tSES: 1.0\n",
      " \tMean Reward: -200.0\n",
      " \tStd Reward: 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                | 2/1001 [00:07<1:06:21,  3.99s/it]\u001b[A[W collection.cpp:602] Warning: torch/profiler/profiler.py(161): stop_trace (function operator())\n",
      "[W collection.cpp:602] Warning: torch/autograd/profiler.py(291): __exit__ (function operator())\n",
      "[W collection.cpp:602] Warning: <built-in method _disable_profiler of PyCapsule object at 0x1207f2700> (function operator())\n",
      "\n",
      "  0%|▏                                               | 3/1001 [00:11<1:06:24,  3.99s/it]\u001b[A\n",
      "  0%|▏                                               | 4/1001 [00:15<1:06:15,  3.99s/it]\u001b[A\n",
      "  0%|▏                                               | 5/1001 [00:20<1:06:50,  4.03s/it]\u001b[A\n",
      "  1%|▎                                               | 6/1001 [00:24<1:06:23,  4.00s/it]\u001b[A[W collection.cpp:602] Warning: torch/profiler/profiler.py(161): stop_trace (function operator())\n",
      "[W collection.cpp:602] Warning: torch/autograd/profiler.py(291): __exit__ (function operator())\n",
      "[W collection.cpp:602] Warning: <built-in method _disable_profiler of PyCapsule object at 0x1207f2700> (function operator())\n",
      "\n",
      "  1%|▎                                               | 7/1001 [00:28<1:06:28,  4.01s/it]\u001b[A\n",
      "  0%|                                                            | 0/12 [00:28<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m tqdm(agents):\n\u001b[1;32m     21\u001b[0m     curriculum \u001b[38;5;241m=\u001b[39m experiments\u001b[38;5;241m.\u001b[39mget_curriculum(agent)\n\u001b[0;32m---> 22\u001b[0m     mountain_car\u001b[38;5;241m.\u001b[39mtrain_evaluate(agent, curriculum)\n\u001b[1;32m     24\u001b[0m q_learning_df \u001b[38;5;241m=\u001b[39m data_frame_from_agents(agents)\n",
      "File \u001b[0;32m~/Workspace/Education/University/KMA/Research/aclarel/environments/mountain_car/environment.py:113\u001b[0m, in \u001b[0;36mtrain_evaluate\u001b[0;34m(agent, curriculum, use_render)\u001b[0m\n\u001b[1;32m    111\u001b[0m total_evaluations \u001b[38;5;241m=\u001b[39m total_episodes \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m evaluation_interval\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m evaluation \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(total_evaluations \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m--> 113\u001b[0m     agent, aar, ses, learning_stability \u001b[38;5;241m=\u001b[39m train_agent(training_env, agent, curriculum, evaluation, total_evaluations)\n\u001b[1;32m    114\u001b[0m     mean_reward, std_reward, total_reward, success_rate \u001b[38;5;241m=\u001b[39m evaluate_agent(evaluation_env, agent, use_render)\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m evaluation \u001b[38;5;241m%\u001b[39m print_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Workspace/Education/University/KMA/Research/aclarel/environments/mountain_car/environment.py:83\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(env, agent, curriculum, evaluation, total_evaluations)\u001b[0m\n\u001b[1;32m     81\u001b[0m action, is_exploratory \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state, greedily\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     82\u001b[0m next_state, reward, done, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 83\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain(state, action, reward, next_state, done)\n\u001b[1;32m     84\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     86\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[0;32m~/Workspace/Education/University/KMA/Research/aclarel/environments/mountain_car/rl_methods/q_learning.py:68\u001b[0m, in \u001b[0;36mQLearningAgent.train\u001b[0;34m(self, prev_state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, prev_state, action, reward, next_state, done):\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m profile(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprofiler_settings) \u001b[38;5;28;01mas\u001b[39;00m prof:\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     71\u001b[0m         prev_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([get_discrete_state(prev_state)], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "File \u001b[0;32m~/miniconda3/envs/aclarel_3.11.3/lib/python3.11/site-packages/torch/profiler/profiler.py:624\u001b[0m, in \u001b[0;36mprofile.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n\u001b[1;32m    625\u001b[0m     prof\u001b[38;5;241m.\u001b[39mKinetoStepTracker\u001b[38;5;241m.\u001b[39merase_step_count(PROFILER_STEP_NAME)\n",
      "File \u001b[0;32m~/miniconda3/envs/aclarel_3.11.3/lib/python3.11/site-packages/torch/profiler/profiler.py:638\u001b[0m, in \u001b[0;36mprofile.stop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord_steps \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_rec_fn:\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_rec_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 638\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transit_action(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_action, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/aclarel_3.11.3/lib/python3.11/site-packages/torch/profiler/profiler.py:666\u001b[0m, in \u001b[0;36mprofile._transit_action\u001b[0;34m(self, prev_action, current_action)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action_list:\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m action_list:\n\u001b[0;32m--> 666\u001b[0m         action()\n",
      "File \u001b[0;32m~/miniconda3/envs/aclarel_3.11.3/lib/python3.11/site-packages/torch/profiler/profiler.py:163\u001b[0m, in \u001b[0;36m_KinetoProfile.stop_trace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstop_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/aclarel_3.11.3/lib/python3.11/site-packages/torch/autograd/profiler.py:296\u001b[0m, in \u001b[0;36mprofile.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_cuda:\n\u001b[1;32m    295\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkineto_results \u001b[38;5;241m=\u001b[39m _disable_profiler()\n\u001b[1;32m    297\u001b[0m _run_on_profiler_stop()\n\u001b[1;32m    298\u001b[0m parsed_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_kineto_results(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkineto_results)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from environments.mountain_car.rl_methods.q_learning import QLearningAgent\n",
    "\n",
    "q_learning_agent = functools.partial(experiments.get_agent, agent_name='q-learning')\n",
    "\n",
    "agents = [\n",
    "    q_learning_agent(curriculum_name='baseline'),\n",
    "    q_learning_agent(curriculum_name='transfer-learning'),\n",
    "    q_learning_agent(curriculum_name='teacher-learning'),\n",
    "    q_learning_agent(curriculum_name='one-pass'),\n",
    "    q_learning_agent(curriculum_name='root-p'),\n",
    "    q_learning_agent(curriculum_name='hard'),\n",
    "    q_learning_agent(curriculum_name='linear'),\n",
    "    q_learning_agent(curriculum_name='logarithmic'),\n",
    "    q_learning_agent(curriculum_name='logistic'),\n",
    "    q_learning_agent(curriculum_name='mixture'),\n",
    "    q_learning_agent(curriculum_name='polynomial'),\n",
    "    q_learning_agent(curriculum_name='anti-curriculum')\n",
    "]\n",
    "\n",
    "for agent in tqdm(agents):\n",
    "    curriculum = experiments.get_curriculum(agent)\n",
    "    mountain_car.train_evaluate(agent, curriculum)\n",
    "\n",
    "q_learning_df = data_frame_from_agents(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f4ad78-e735-456b-9620-b119a486bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learning_df.to_csv(DATASETS_DIR / 'q-learning.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad6e92a-49c0-4c9f-9ca9-2dfad2290c98",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381359f6-ce78-48fc-b4b4-450891ecf863",
   "metadata": {},
   "source": [
    "#### Curriculum parameter: gravity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29c9f1e-b0ac-4375-878f-4a6eff425452",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from environments.mountain_car.rl_methods.dqn import DQNAgent\n",
    "\n",
    "dqn_agent = functools.partial(experiments.get_agent, agent_name='dqn')\n",
    "\n",
    "agents = [\n",
    "    dqn_agent(curriculum_name='baseline'),\n",
    "    dqn_agent(curriculum_name='transfer-learning'),\n",
    "    dqn_agent(curriculum_name='teacher-learning'),\n",
    "    dqn_agent(curriculum_name='one-pass'),\n",
    "    dqn_agent(curriculum_name='root-p'),\n",
    "    dqn_agent(curriculum_name='hard'),\n",
    "    dqn_agent(curriculum_name='linear'),\n",
    "    dqn_agent(curriculum_name='logarithmic'),\n",
    "    dqn_agent(curriculum_name='logistic'),\n",
    "    dqn_agent(curriculum_name='mixture'),\n",
    "    dqn_agent(curriculum_name='polynomial'),\n",
    "    dqn_agent(curriculum_name='anti-curriculum')\n",
    "]\n",
    "\n",
    "for agent in tqdm(agents):\n",
    "    curriculum = experiments.get_curriculum(agent)\n",
    "    cart_pole.train_evaluate(agent, curriculum)\n",
    "\n",
    "dqn_df = data_frame_from_agents(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b2cf0a-776a-4c0f-b564-82e866fdbecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_df.to_csv(DATASETS_DIR / 'dqn.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aclarel_3.11.3",
   "language": "python",
   "name": "aclarel_3.11.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
